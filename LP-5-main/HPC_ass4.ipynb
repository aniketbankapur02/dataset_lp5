{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!nvcc --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SFbuN-RpsVVg",
        "outputId": "b18e738a-5f5d-4827-b58c-1628d7c31e25"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2022 NVIDIA Corporation\n",
            "Built on Wed_Sep_21_10:33:58_PDT_2022\n",
            "Cuda compilation tools, release 11.8, V11.8.89\n",
            "Build cuda_11.8.r11.8/compiler.31833905_0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/andreinechaev/nvcc4jupyter.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Md-tOEumsaHL",
        "outputId": "e2b43b57-e13e-44f7-922b-3afb43615cad"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/andreinechaev/nvcc4jupyter.git\n",
            "  Cloning https://github.com/andreinechaev/nvcc4jupyter.git to /tmp/pip-req-build-cz0tn_qr\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/andreinechaev/nvcc4jupyter.git /tmp/pip-req-build-cz0tn_qr\n",
            "  Resolved https://github.com/andreinechaev/nvcc4jupyter.git to commit aac710a35f52bb78ab34d2e52517237941399eff\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: NVCCPlugin\n",
            "  Building wheel for NVCCPlugin (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for NVCCPlugin: filename=NVCCPlugin-0.0.2-py3-none-any.whl size=4287 sha256=194a071cdce43ec0da1bcd2a72836d9047a5ac7365efa00d8deb8b1bd157a2d7\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-00_2ab5x/wheels/a8/b9/18/23f8ef71ceb0f63297dd1903aedd067e6243a68ea756d6feea\n",
            "Successfully built NVCCPlugin\n",
            "Installing collected packages: NVCCPlugin\n",
            "Successfully installed NVCCPlugin-0.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext nvcc_plugin"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6sjp-HsVsccd",
        "outputId": "02e6693c-54b1-463b-b6b4-235076df62a3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "created output directory at /content/src\n",
            "Out bin /content/result.out\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## VECTOR ADDITION"
      ],
      "metadata": {
        "id": "Hi_oNCMh-7Ti"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%cu\n",
        "\n",
        "#include <stdio.h>\n",
        "\n",
        "// CUDA kernel for vector addition\n",
        "__global__ void vectorAdd(int* a, int* b, int* c, int size) \n",
        "{\n",
        "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (tid < size) {\n",
        "        c[tid] = a[tid] + b[tid];\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() \n",
        "{\n",
        "    int size = 100;  // Size of the vectors\n",
        "    int* a, * b, * c;    // Host vectors\n",
        "    int* dev_a, * dev_b, * dev_c;  // Device vectors\n",
        "\n",
        "    // Allocate memory for host vectors\n",
        "    a = (int*)malloc(size * sizeof(int));\n",
        "    b = (int*)malloc(size * sizeof(int));\n",
        "    c = (int*)malloc(size * sizeof(int));\n",
        "\n",
        "    // Initialize host vectors\n",
        "    for (int i = 0; i < size; i++) {\n",
        "        a[i] = i;\n",
        "        b[i] = 2 * i;\n",
        "    }\n",
        "\n",
        "    // Allocate memory on the device for device vectors\n",
        "    cudaMalloc((void**)&dev_a, size * sizeof(int));\n",
        "    cudaMalloc((void**)&dev_b, size * sizeof(int));\n",
        "    cudaMalloc((void**)&dev_c, size * sizeof(int));\n",
        "\n",
        "    // Copy host vectors to device\n",
        "    cudaMemcpy(dev_a, a, size * sizeof(int), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(dev_b, b, size * sizeof(int), cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Launch kernel for vector addition\n",
        "    int blockSize = 256;\n",
        "    int gridSize = (size + blockSize - 1) / blockSize;\n",
        "    vectorAdd<<<gridSize, blockSize>>>(dev_a, dev_b, dev_c, size);\n",
        "\n",
        "    // Copy result from device to host\n",
        "    cudaMemcpy(c, dev_c, size * sizeof(int), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // Print result\n",
        "    for (int i = 0; i < size; i++) {\n",
        "        printf(\"%d + %d = %d\\n\", a[i], b[i], c[i]);\n",
        "    }\n",
        "\n",
        "    // Free device memory\n",
        "    cudaFree(dev_a);\n",
        "    cudaFree(dev_b);\n",
        "    cudaFree(dev_c);\n",
        "\n",
        "    // Free host memory\n",
        "    free(a);\n",
        "    free(b);\n",
        "    free(c);\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oo8J4IoZm1c4",
        "outputId": "7882ef60-1a10-4cce-d51e-fa484465e83d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 + 0 = 0\n",
            "1 + 2 = 3\n",
            "2 + 4 = 6\n",
            "3 + 6 = 9\n",
            "4 + 8 = 12\n",
            "5 + 10 = 15\n",
            "6 + 12 = 18\n",
            "7 + 14 = 21\n",
            "8 + 16 = 24\n",
            "9 + 18 = 27\n",
            "10 + 20 = 30\n",
            "11 + 22 = 33\n",
            "12 + 24 = 36\n",
            "13 + 26 = 39\n",
            "14 + 28 = 42\n",
            "15 + 30 = 45\n",
            "16 + 32 = 48\n",
            "17 + 34 = 51\n",
            "18 + 36 = 54\n",
            "19 + 38 = 57\n",
            "20 + 40 = 60\n",
            "21 + 42 = 63\n",
            "22 + 44 = 66\n",
            "23 + 46 = 69\n",
            "24 + 48 = 72\n",
            "25 + 50 = 75\n",
            "26 + 52 = 78\n",
            "27 + 54 = 81\n",
            "28 + 56 = 84\n",
            "29 + 58 = 87\n",
            "30 + 60 = 90\n",
            "31 + 62 = 93\n",
            "32 + 64 = 96\n",
            "33 + 66 = 99\n",
            "34 + 68 = 102\n",
            "35 + 70 = 105\n",
            "36 + 72 = 108\n",
            "37 + 74 = 111\n",
            "38 + 76 = 114\n",
            "39 + 78 = 117\n",
            "40 + 80 = 120\n",
            "41 + 82 = 123\n",
            "42 + 84 = 126\n",
            "43 + 86 = 129\n",
            "44 + 88 = 132\n",
            "45 + 90 = 135\n",
            "46 + 92 = 138\n",
            "47 + 94 = 141\n",
            "48 + 96 = 144\n",
            "49 + 98 = 147\n",
            "50 + 100 = 150\n",
            "51 + 102 = 153\n",
            "52 + 104 = 156\n",
            "53 + 106 = 159\n",
            "54 + 108 = 162\n",
            "55 + 110 = 165\n",
            "56 + 112 = 168\n",
            "57 + 114 = 171\n",
            "58 + 116 = 174\n",
            "59 + 118 = 177\n",
            "60 + 120 = 180\n",
            "61 + 122 = 183\n",
            "62 + 124 = 186\n",
            "63 + 126 = 189\n",
            "64 + 128 = 192\n",
            "65 + 130 = 195\n",
            "66 + 132 = 198\n",
            "67 + 134 = 201\n",
            "68 + 136 = 204\n",
            "69 + 138 = 207\n",
            "70 + 140 = 210\n",
            "71 + 142 = 213\n",
            "72 + 144 = 216\n",
            "73 + 146 = 219\n",
            "74 + 148 = 222\n",
            "75 + 150 = 225\n",
            "76 + 152 = 228\n",
            "77 + 154 = 231\n",
            "78 + 156 = 234\n",
            "79 + 158 = 237\n",
            "80 + 160 = 240\n",
            "81 + 162 = 243\n",
            "82 + 164 = 246\n",
            "83 + 166 = 249\n",
            "84 + 168 = 252\n",
            "85 + 170 = 255\n",
            "86 + 172 = 258\n",
            "87 + 174 = 261\n",
            "88 + 176 = 264\n",
            "89 + 178 = 267\n",
            "90 + 180 = 270\n",
            "91 + 182 = 273\n",
            "92 + 184 = 276\n",
            "93 + 186 = 279\n",
            "94 + 188 = 282\n",
            "95 + 190 = 285\n",
            "96 + 192 = 288\n",
            "97 + 194 = 291\n",
            "98 + 196 = 294\n",
            "99 + 198 = 297\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MATRIX MULTIPLICATION"
      ],
      "metadata": {
        "id": "4QH_TMzA_AiB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%cu\n",
        "\n",
        "#include <stdio.h>\n",
        "\n",
        "// CUDA kernel for matrix multiplication\n",
        "__global__ void matrixMul(int* a, int* b, int* c, int rowsA, int colsA, int colsB) {\n",
        "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    int sum = 0;\n",
        "    if (row < rowsA && col < colsB) {\n",
        "        for (int i = 0; i < colsA; i++) {\n",
        "            sum += a[row * colsA + i] * b[i * colsB + col];\n",
        "        }\n",
        "        c[row * colsB + col] = sum;\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int rowsA = 10;  // Rows of matrix A\n",
        "    int colsA = 10;  // Columns of matrix A\n",
        "    int rowsB = colsA; // Rows of matrix B\n",
        "    int colsB = 10;  // Columns of matrix B\n",
        "\n",
        "    int* a, * b, * c;  // Host matrices\n",
        "    int* dev_a, * dev_b, * dev_c;  // Device matrices\n",
        "\n",
        "    // Allocate memory for host matrices\n",
        "    a = (int*)malloc(rowsA * colsA * sizeof(int));\n",
        "    b = (int*)malloc(rowsB * colsB * sizeof(int));\n",
        "    c = (int*)malloc(rowsA * colsB * sizeof(int));\n",
        "\n",
        "    // Initialize host matrices\n",
        "    for (int i = 0; i < rowsA * colsA; i++) {\n",
        "        a[i] = i;\n",
        "    }\n",
        "    for (int i = 0; i < rowsB * colsB; i++) {\n",
        "        b[i] = 2 * i;\n",
        "    }\n",
        "\n",
        "    // Allocate memory on the device for device matrices\n",
        "    cudaMalloc((void**)&dev_a, rowsA * colsA * sizeof(int));\n",
        "    cudaMalloc((void**)&dev_b, rowsB * colsB * sizeof(int));\n",
        "    cudaMalloc((void**)&dev_c, rowsA * colsB * sizeof(int));\n",
        "\n",
        "    // Copy host matrices to device\n",
        "    cudaMemcpy(dev_a, a, rowsA * colsA * sizeof(int), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(dev_b, b, rowsB * colsB * sizeof(int), cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Define grid and block dimensions\n",
        "    dim3 blockSize(16, 16);\n",
        "    dim3 gridSize((colsB + blockSize.x - 1) / blockSize.x, (rowsA + blockSize.y - 1) / blockSize.y);\n",
        "\n",
        "    // Launch kernel for matrix multiplication\n",
        "    matrixMul<<<gridSize, blockSize>>>(dev_a, dev_b, dev_c, rowsA, colsA, colsB);\n",
        "\n",
        "    // Copy result from device to host\n",
        "    cudaMemcpy(c, dev_c, rowsA * colsB * sizeof(int), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // Print result\n",
        "    printf(\"Result:\\n\");\n",
        "    for (int i = 0; i < rowsA; i++) {\n",
        "        for (int j = 0; j < colsB; j++) {\n",
        "            printf(\"%d \", c[i * colsB + j]);\n",
        "        }\n",
        "        printf(\"\\n\");\n",
        "    }\n",
        "\n",
        "    // Free device memory\n",
        "    cudaFree(dev_a);\n",
        "    cudaFree(dev_b);\n",
        "    cudaFree(dev_c);\n",
        "\n",
        "    // Free host memory\n",
        "    free(a);\n",
        "    free(b);\n",
        "    free(c);\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yl6vJq3im1as",
        "outputId": "8b04082a-027a-415e-8f12-d1141cb4e0a6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result:\n",
            "5700 5790 5880 5970 6060 6150 6240 6330 6420 6510 \n",
            "14700 14990 15280 15570 15860 16150 16440 16730 17020 17310 \n",
            "23700 24190 24680 25170 25660 26150 26640 27130 27620 28110 \n",
            "32700 33390 34080 34770 35460 36150 36840 37530 38220 38910 \n",
            "41700 42590 43480 44370 45260 46150 47040 47930 48820 49710 \n",
            "50700 51790 52880 53970 55060 56150 57240 58330 59420 60510 \n",
            "59700 60990 62280 63570 64860 66150 67440 68730 70020 71310 \n",
            "68700 70190 71680 73170 74660 76150 77640 79130 80620 82110 \n",
            "77700 79390 81080 82770 84460 86150 87840 89530 91220 92910 \n",
            "86700 88590 90480 92370 94260 96150 98040 99930 101820 103710 \n",
            "\n"
          ]
        }
      ]
    }
  ]
}
